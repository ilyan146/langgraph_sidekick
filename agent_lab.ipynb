{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4420b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages # A reducer function\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, Optional, Any\n",
    "from databricks_langchain import ChatDatabricks\n",
    "import os\n",
    "from langgraph_sidekick.client import AzureAIClient\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel, Field\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "import aiosqlite\n",
    "from langgraph_sidekick.agent_tools import get_agent_tools, get_playwright_tools\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "import uuid\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ae566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple state\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Get llms\n",
    "llm_db = ChatDatabricks(endpoint=\"databricks-claude-3-7-sonnet\", max_tokens=1000)\n",
    "\n",
    "llm_az = AzureChatOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_ad_token_provider=AzureAIClient().token_provider,\n",
    "    azure_deployment=\"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tools\n",
    "tools_list = await get_agent_tools()\n",
    "playwright_tools, browser, playwright = await get_playwright_tools()\n",
    "\n",
    "agent_tools = tools_list + playwright_tools\n",
    "print(agent_tools)\n",
    "\n",
    "# Give the llm the tools\n",
    "llm_with_tools = llm_az.bind_tools(agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050717d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in agent_tools:\n",
    "    print(f\"Tool Name: {tool.name} == {tool}\")\n",
    "\n",
    "# Drop the serper tool for testing the playwright tool\n",
    "agent_tools = [tool for tool in agent_tools if tool.name != \"google_serper_search_tool\"]\n",
    "print(\"\\n\\n\", agent_tools)\n",
    "for tool in agent_tools:\n",
    "    print(f\"Tool Name: {tool.name} == {tool}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0390347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory database\n",
    "db_path=\"memory_db/sqlite_memory.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "async def setup_async_db():\n",
    "    async_conn = await aiosqlite.connect(db_path)\n",
    "    return async_conn\n",
    "\n",
    "async_conn = await setup_async_db()\n",
    "\n",
    "# sql_memory = SqliteSaver(conn)\n",
    "sql_memory = AsyncSqliteSaver(async_conn)\n",
    "\n",
    "# memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7add86f",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function\n",
    "def chat(state: State) -> State:\n",
    "    response = llm_with_tools.invoke(state.messages)\n",
    "    new_state = State(messages=[response])\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(state_schema=State)\n",
    "\n",
    "graph_builder.add_edge(START, \"chat\")\n",
    "graph_builder.add_node(\"chat\", chat)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=agent_tools))\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chat\", tools_condition, \"tools\")\n",
    "graph_builder.add_edge(\"tools\", \"chat\")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "# graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958cb20",
   "metadata": {},
   "source": [
    "### Create gradio chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "async def gradio_chat(user_input: str, history):\n",
    "    state = State(messages=[{\"role\": \"user\", \"content\": user_input}])\n",
    "    response = await graph.ainvoke(state, config=config) # Config sets the thread to use in memory\n",
    "    print(response)\n",
    "\n",
    "    return response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa847aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat interface\n",
    "gr.ChatInterface(gradio_chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48467d3",
   "metadata": {},
   "source": [
    "## Adding another Evaluator agent into the mix with some structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarification is required, or the assistant is stuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make  a new state\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ee322",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = llm_db.with_structured_output(EvaluatorOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c853fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First assistant - aka - worker_agent \n",
    "def agent_worker(state: State) -> State:\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "    You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "    This is the success criteria:\n",
    "    {state.success_criteria}\n",
    "    You should reply either with a question for the user about this assignment, or with your final response.\n",
    "    If you have a question for the user, you need to reply by clearly stating your question. \n",
    "    An example Quenstion might be: Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "    If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "    \"\"\"\n",
    "        \n",
    "    if state.feedback_on_work:\n",
    "        system_message += f\"\"\"\n",
    "        Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "        Here is the feedback on why this was rejected:\n",
    "        {state.feedback_on_work}\n",
    "        With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "    \n",
    "    # Add in the system message\n",
    "    found_system_message = False\n",
    "    # messages = state[\"messages\"]\n",
    "    messages = state.messages\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "    \n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    # Invoke the LLM with tools\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    new_state = state.model_copy(update={\"messages\": [response]})\n",
    "    # Return updated state\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ce3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_worker routing - conditional edge that decides whether to route to the evaluator or to using tools\n",
    "def agent_worker_router(state: State) -> str:\n",
    "    last_message = state.messages[-1]\n",
    "    print(f\"Last message type: {type(last_message)}\")\n",
    "    print(f\"Last message content: {last_message}\")\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"agent_evaluator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ee752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the messages into a more followable format for the evalutor agent\n",
    "def format_converstation(messages: list[Any]) -> str:\n",
    "    convo = \"Conversation history:\\n\\n\"\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            convo += f\"User: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            text = msg.content or \"[Tools used]\"\n",
    "            convo += f\"Assistant: {text}\\n\"\n",
    "    return convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second assistant - aka - evaluator_agent\n",
    "def agent_evaluator(state: State) -> State:\n",
    "    last_response = state.messages[-1].content\n",
    "\n",
    "    system_message = \"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "    Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether the success criteria has been met,\n",
    "    and whether more input is needed from the user.\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "    Here is the conversation history:\n",
    "    {format_converstation(state.messages)}\n",
    "\n",
    "    The success criteria for this assignment is:\n",
    "    {state.success_criteria}\n",
    "\n",
    "    The final response from the Assistant is:\n",
    "    {last_response}\n",
    "\n",
    "    Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "    Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "\n",
    "    If the Assistant says they have used the tool you should give the Assistant the benefit of the doubt if they say they've done something. But you should reject if you feel that more work should go into this.\n",
    "    \"\"\"\n",
    "\n",
    "    if state.feedback_on_work:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state.feedback_on_work}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    evaluator_messages  = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "\n",
    "    evaluator_response = evaluator_llm.invoke(evaluator_messages)\n",
    "\n",
    "    new_state = State(\n",
    "        messages=[{\"role\": \"assistant\", \"content\": f\"Evaluator Feedback on this answer: {evaluator_response.feedback}\"}],\n",
    "        feedback_on_work=evaluator_response.feedback,\n",
    "        success_criteria_met=evaluator_response.success_criteria_met,\n",
    "        user_input_needed=evaluator_response.user_input_needed,\n",
    "        success_criteria=state.success_criteria,\n",
    "        )\n",
    "    \n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ed0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing the agent evaluator for the edges\n",
    "\n",
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state.success_criteria_met or state.user_input_needed:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"agent_worker\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "## WE are all set make the graph\n",
    "graph_builder = StateGraph(state_schema=State)\n",
    "\n",
    "# Nodes\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=agent_tools))\n",
    "graph_builder.add_node(\"agent_worker\", agent_worker)\n",
    "graph_builder.add_node(\"agent_evaluator\", agent_evaluator)\n",
    "\n",
    "# Edges \n",
    "graph_builder.add_conditional_edges(\"agent_worker\", agent_worker_router, {\"tools\": \"tools\", \"agent_evaluator\": \"agent_evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"agent_worker\")\n",
    "graph_builder.add_conditional_edges(\"agent_evaluator\", route_based_on_evaluation, {\"agent_worker\": \"agent_worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"agent_worker\")\n",
    "\n",
    "# Compile the graph\n",
    "# graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Display the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final Step Gradio inplementaion\n",
    "\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread}}\n",
    "\n",
    "    # state = {\n",
    "    #     \"messages\": message,\n",
    "    #     \"success_criteria\": success_criteria,\n",
    "    #     \"feedback_on_work\": None,\n",
    "    #     \"success_criteria_met\": False,\n",
    "    #     \"user_input_needed\": False\n",
    "    # }\n",
    "    \n",
    "    state = State(\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        success_criteria=success_criteria,\n",
    "        feedback_on_work=None, # to be set by the evaluator agent\n",
    "        success_criteria_met=False, # to start with its false\n",
    "        user_input_needed=False # To start with its false\n",
    "    )\n",
    "\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    user = {\"role\": \"user\", \"content\": message}\n",
    "    reply = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "    return history + [user, reply, feedback]\n",
    "\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together in a gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success critiera?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed68485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
